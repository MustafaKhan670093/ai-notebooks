# The AI Notebooks

Everyone and their mom's hamster wants to learn AI. A lot of people manage to learn the basics of how backpropagation works by watching some [3B1B videos](https://www.3blue1brown.com/topics/neural-networks) or by completing some courses online from [Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188) or [Coursera](https://www.coursera.org/collections/machine-learning). 

This is my take on teaching myself (and maybe helping others learn) about AI by going through the core ideas in the field that have stood the test of time and presenting them in historical order. AI research does not occur in a vaccum. It occurs as a result of people reacting to, responding to, criticising, and being inspired by the work being done before and around them at any given moment. Most importantly, we need to work directly with papers (where the ideas are presented) and code (where the ideas are actualized).

## 2012: The Era of ImageNet

- [AlexNet](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
- [Dropout](http://arxiv.org/abs/1207.0580)
- [SGD](https://arxiv.org/pdf/1404.5997.pdf)

## 2013: Playing Atari with Deep Reinforcement Learning
- [Reinforcement learning](https://arxiv.org/pdf/1312.5602.pdf)
